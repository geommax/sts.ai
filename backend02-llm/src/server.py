from flask import Flask, request, jsonify
import os
import time

app = Flask(__name__)

@app.route('/')
def index():
    return jsonify({"message": "LLM Backend Server Running"})

@app.route('/generate', methods=['POST'])
def generate_response():
    start_time = time.time()
    
    try:
        data = request.get_json()
        prompt = data.get('prompt', '')
        
        # Simulate LLM processing time
        time.sleep(0.8)  # Simulate 800ms processing time for a more realistic LLM
        
        # Calculate processing time
        processing_time = time.time() - start_time
        
        # Placeholder for LLM implementation
        # In a real implementation, you would:
        # 1. Load your LLM model
        # 2. Process the prompt
        # 3. Generate a response
        
        response_text = f"This is a placeholder response to: '{prompt}'. In a full implementation, this would be generated by an LLM. (Processed in {round(processing_time*1000)}ms)"
        
        return jsonify({
            "response": response_text,
            "latency": {
                "processing": round(processing_time * 1000)  # Convert to milliseconds
            }
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5002, debug=True)